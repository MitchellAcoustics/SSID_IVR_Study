---
title: A Brand New Title, Very Natural Saying Visual Perception Drives Soundscape Perception By A Lot
author:
  - name: Tin Oberman
    orcid: 0000-0002-0014-0383
    corresponding: true
    email: t.oberman@ucl.ac.uk
    roles:
      - Investigation
      - Project administration
      - Visualization
      - Writing
      - Experimental design
      - Experiment setup
      - Conducting the experiments
    affiliations:
      - University College London
  - name: Andrew Mitchell
    orcid: 0000-0003-0978-5046
    corresponding: false
    roles:
     - Statistical analysis
     - Coding
     - Experimental design
    affiliations:
      - University College London
  - name: Yuqi Liang
    orcid: 0009-0004-8276-5169
    corresponding: false
    roles:
    - Data analysis
    - Writing
    - Revisions
    affiliations:
      - University College London
  - name: Francesco Aletta
    orcid: 0000-0003-0351-3189
    corresponding: false
    roles:
     - Experimental design
     - Revisions
    affiliations:
     - University College London
  - name: Jian Kang
    orcid: 0000-0001-8995-5636
    corresponding: false
    roles:
     - Experimental design
     - Project administration
     - Securing funding
    affiliations: 
     - University College London
keywords:
  - La Palma
  - Earthquakes
  - Soundscape
  - Immersive Virtual Reality
  - Eye Tracking
abstract: |
  This study investigates the effect of visual context on soundscape perception in an urban environment. The experiment was conducted in an immersive virtual reality environment, where participants were exposed to audio and visual stimuli. The results show that visual context significantly influences the perceived affective quality of the soundscape. The findings have implications for soundscape design and urban planning.
  Perception of sound environments is influenced by the context we experience them in. A big portion of the contextual information comes from the visual domain, including our understanding of what a place is but also from its visual features. A laboratory study combining questionnaire responses and eye-tracking tools was designed to investigate if the soundscape outcomes and participants’ behaviour inside the simulation can be explained by the perceptual outcomes defined by visual information. 360 degree videos and First Order Ambisonics audio recordings of 27 different urban open spaces taken from the International Soundscape Database were used as the stimuli delivered via a Virtual Reality Head-Mounted Display and a Higher Order Ambisonics speaker array, while a neutral grey environment without sounds being reproduced represented the baseline scenario. A questionnaire tool was deployed within the IVR simulation to collect participants’ responses describing their perception of the reproduced environments corresponding to the circumplex model featured in the Method A of the ISO/TS 12913-2. The results revealed a good coverage of the two-dimensional perceptual circumplex space and significant differences between perceptual outcomes driven by sound and those driven by visual stimuli. 
date: last-modified
number-sections: true
---

# Introduction


Making a change here.

Based on data up to and including 1971 @Mitchell2022How, eruptions on La Palma happen every  years on average.

Studies of the magma systems feeding the volcano, such as @marrero2019, have proposed that there are two main magma reservoirs feeding the Cumbre Vieja volcano; one in the mantle (30-40km depth) which charges and in turn feeds a shallower crustal reservoir (10-20km depth).

Eight eruptions have been recorded since the late 1400s (@fig-timeline).

Data and methods are discussed in @sec-data-methods.

Let $x$ denote the number of eruptions in a year. Then, $x$ can be modeled by a Poisson distribution

$$
p(x) = \frac{e^{-\lambda} \lambda^{x}}{x !}
$$ {#eq-poisson}

where $\lambda$ is the rate of eruptions per year. Using @eq-poisson, the probability of an eruption in the next $t$ years can be calculated.

| Name                | Year |
|---------------------|------|
| Current             | 2021 |
| Teneguía            | 1971 |
| Nambroque           | 1949 |
| El Charco           | 1712 |
| Volcán San Antonio  | 1677 |
| Volcán San Martin   | 1646 |
| Tajuya near El Paso | 1585 |
| Montaña Quemada     | 1492 |

: Recent historic eruptions on La Palma {#tbl-history}

@tbl-history summarises the eruptions recorded since the colonization of the islands by Europeans in the late 1400s.

![Map of La Palma](figures/la-palma-map.png){#fig-map}

La Palma is one of the west most islands in the Volcanic Archipelago of the Canary Islands (@fig-map).

{{< embed notebooks/data-screening.ipynb#fig-spatial-plot >}}

@fig-spatial-plot shows the location of recent Earthquakes on La Palma.

# Results
## Perception

The preliminary results reveal a good coverage of the two-dimensional perceptual circumplex space and significant differences between perceptual outcomes driven by sound and those driven by visual stimuli. The perceptual outcomes in the two-dimensional circumplex space were calculated from the eight scales, relying on the recommendations outlined in the ISO/TS 12913-3(13) and visualised in the Figure 2 using the Soundscapy tool (8)

## Behaviour
## Effect of visual context
# Discussion
# Data & Methods {#sec-data-methods}
## Ethical statement
## Participants
## Task

The experiment consisted of three blocks within which the 27 stimuli were presented in random order, including a neutral grey environment without any sounds being reproduced which represented the baseline scenario. First block featured only auditory stimuli with a neutral grey environment presented in the HMD, the second block featured only 15s long video samples similar to (12), while the third block featured both audio and visual recordings combined. The participants were advised to take frequent breaks, one within each block but more were allowed upon request.

## Laboratory setup

The experiment was conducted in a listening chamber featuring twelve coaxial active speakers (Genelec 8331) arranged to provide Second Order Ambisonic playback, while the visual stimuli were delivered via an Immersive Virtual Reality (IVR) Head-Mounted Display (HMD), as shown in Figure 1. The participant was required to be sitting on a highchair with their head in a “sweet spot” – as close as possible to the point that is equally distant from the centre of all the twelve speakers.

The questionnaire was deployed within the IVR simulation created in Unity and operated by a participant via joystick.

## Pilot experiment

Two pilot experiments were conducted: 1) Investigating the applicability of the Method A questionnaire to assess the perceived affective quality of the video scenes without the audio stimuli (12); 2) Optimising the stimuli selection to cover the full circumplex model associated with the eight dimensions employed in the Method A.

The first pilot experiment featured 39 different scenes and revealed a good coverage of the calm and eventful space. The annoying and uneventful dimensions were not covered which could mean that either visual context cannot generate a monotonous outcome in an urban space, or that the dataset is missing more stimuli that would address that segment of the circumplex space (12).

This led to further twelve field recordings made specifically to target the annoying, monotonous and uneventful area of the circumplex model.

### Stimuli selection

The second pilot study was focused on providing three scenes per each section of the circumplex space in a three by three matrix. Three researchers conducted independent assessments of the 51 videos (the combined audio and video condition) to select the most representative 27 stimuli. After the independent assessment, the outcome was discussed until an agreement was reached. The selected scenes are presented in Table 1.

## Main experiment
## Data & Methods {#sec-data-methods}

### Video Processing
The method for video processing involves a systematic approach to process video frames, overlay gaze coordinates, and output processed videos. The following steps were undertaken:

1. Participant Data Preparation
The eyetracking data for participants, including timestamps, gaze coordinates and annotations for menu, was collected and stored by iMotion in CSV files. The timestamps is in milliseconds, while the gaze coordinates (Gaze X, Gaze Y) are measured from the top-left corner of the video frame.

2. Video Format Conversion
The input videos were checked to ensure compatibility with processing tools. Videos in non-MP4 formats, such as WMV, were converted to MP4 to ensure consistency and avoid compatibility issues.

3. Processing Videos
The `processing_video` function to process and delete the video frames with menu, and overlay the coordinates in the frames. The process included:
    - Reading video metadata (e.g., FPS and total number of frames) and gaze data (Gaze X, Gaze Y).
    - Iteratively reading and processing the video frames and determining the current timestamp, then chopping the frames with annotation.
    - If the frame not going to skip, then matching it with the corrresponding gaze data, then drawing colored circles on the video frames based on the gaze coordinates.
    - Saving the resulting videos to a specified MP4 output path.
  


#### Implementation
The entire process was implemented in Python using pandas for handling eyetracking data, the OpenCV library for video processing and ffmpeg for video format conversion The workflow ensured that all steps were reproducible and adaptable to different datasets in csv format.



## Concluding

Adding some new lines of text to test collaboration. And more comments

# References {.unnumbered}

::: {#refs}
:::

# Acknowledgements

This research was funded through the European Research Council (ERC) Advanced Grant (no.740696) on “Soundscape Indices” (SSID). We want to thank Ms Xinrui Xu, Mr Daniel Perski and Ms Hui-Zhong Zhang for their support with data collection and experimental setup.
